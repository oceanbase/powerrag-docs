# Observability data model

In PowerRAG, tracing is the fundamental mechanism for implementing observability. By recording and analyzing the execution process of Large Language Model (LLM) applications, it helps developers understand system runtime status, performance bottlenecks, and model behavior.  

PowerRAG's data model references OpenTelemetry's design philosophy and extends it based on the characteristics of large model applications, enabling complete characterization of the entire process of a model interaction.

## Trace and observation

### Trace

A trace typically represents a complete request or operation. It contains the overall input and output information of functions or tasks, as well as related contextual metadata (such as user identifiers, session IDs, tags, request sources, etc.).

In PowerRAG, trace is the top-level structure of all observability data, used to uniformly represent a complete model call, Agent execution, or application request.

### Observation

A trace can contain multiple observations, used to record various steps in the execution process. Typically, an observation corresponds to a specific call or execution node in the application, such as model generation, tool calls, or context retrieval.

#### Nesting structure

Observations support nested structures. You can manually create nested relationships, or automatically implement them using OpenTelemetry's context propagation mechanism. This nested structure can clearly reflect the hierarchical relationships in the trace, such as multiple model calls and tool calls within a main request.

#### Observation types

PowerRAG supports multiple Observation types suitable for LLM application scenarios, used to more precisely describe different execution stages:

| **Type** | **Description** |
| --- | --- |
| event | The most basic event unit, used to record discrete events (such as state changes, trigger points, etc.). |
| span | Represents a work unit with clear start and end times, used to measure execution duration. |
| generation | Records detailed information about large model generation tasks, including prompts, token usage, latency, and costs. |
| agent | Represents decision-making and flow control nodes of an agent, which can include tool calls, model inference, and other operations. |
| tool | Represents tool calls, such as external API calls (e.g., weather services or retrieval tools). |
| chain | Represents associations between application steps, such as data transfer links from retrieval modules to model calls. |
| retriever | Represents data retrieval operations, such as vector database queries or database access. |
| evaluator | Represents evaluation functions used to assess model output quality, relevance, or correctness. |
| embedding | Represents the call process for generating embedding vectors, including model, token usage, and cost information. |
| guardrail | Represents security guardrail components, used to identify and prevent harmful content or jailbreak attacks. |

Through these types, developers can build complete multi-layer trace structures and precisely analyze application execution logic and model performance.

## Session

Multiple traces can belong to the same session. Sessions are used to aggregate multiple requests belonging to the same user interaction or context. For example:

+ A conversation thread in a chat application
+ Multiple rounds of calls in continuous Q&A
+ Multiple model calls in the same task flow

Through sessions, PowerRAG can help you analyze the overall performance of models in a complete interaction at a macro level.

## Score

Score is a universal object in PowerRAG's data model used to quantitatively evaluate trace, observation, session, or dataset run results. Scores can be numeric, categorical, or boolean, and support flexible configuration and validation. Typical features include:

+ Can be bound to one of trace, session, or dataset run.
+ Trace-level scores can be further associated with a specific observation (optional).
+ Supports adding comments to provide contextual explanations.
+ Can be validated through predefined score configuration schemas.

Common use cases include:

| **Scenario** | **Usage Example** |
| --- | --- |
| Session-level scoring | Comprehensive evaluation of coherence, relevance, or experience of the entire multi-turn conversation. |
| Trace-level scoring | Evaluation of accuracy or reasonableness of a single model output. |
| Dataset run-level scoring | Statistics on the overall effectiveness of the entire dataset run, such as precision, recall, F1-score, etc. |

Through the score mechanism, PowerRAG can not only record execution data but also provide quantifiable metrics for model optimization and effectiveness evaluation.

