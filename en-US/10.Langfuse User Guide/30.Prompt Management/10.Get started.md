# Get started

This quickstart helps you to create your first prompt and use it in your application.

### Get API keys

1. Log in to the Langfuse console.
2. In the left-side navigation pane, click **Settings**. Click **API Keys** on the page. 
3. Create and obtain a pair of new API keys, including a public key and a private key.

## Create a prompt

### Langfuse UI

Use the Langfuse UI to create a new prompt or update an existing one.

1. In the left-side navigation pane, click **Prompts**. Click **+ New prompt** in the upper-right corner of the page.
2. Configure the prompt parameters. For example:
    - Name: `movie-critic`
    - Prompt text: `As a {{criticlevel}} movie critic, do you like {{movie}}?`
    - Config: `{"model": "gpt-4o"}`
    - Labels: Select **Set the "production" label**, which indicates to promote the prompt to production once it is created.
3. Click **Create prompt**.
4. After creation, you are redirected to the prompt details page. You can click **+ New** on the **Versions** tab to update the prompt. Click **Save new prompt version** after modification.
5. If you have not selected the ‚Äúproduction‚Äù label and need to promote it to production. Click the ![](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/powerrag/langfuse/icon1.png) icon next to the version in the version list of the prompt details page. Select **‚Äúproduction‚Äù** and click **Save**.

### Python SDK

1. Install SDK.

```bash
pip install langfuse
```

2. Add your Langfuse keys as environment variables.

```bash filename=".env"
LANGFUSE_SECRET_KEY = "sk-lf-..."
LANGFUSE_PUBLIC_KEY = "pk-lf-..."
LANGFUSE_HOST = "https://xxx"
```

3. Use the Python SDK to create a new prompt or update an existing one.

```python
# Create a text prompt
langfuse.create_prompt(
    name="movie-critic",
    type="text",
    prompt="As a {{criticlevel}} movie critic, do you like {{movie}}?",
    labels=["production"],  # directly promote to production
    config={
        "model": "gpt-4o",
        "temperature": 0.7,
        "supported_languages": ["en", "fr"],
    },  # optionally, add configs (e.g. model parameters or model tools) or tags
)

# Create a chat prompt
langfuse.create_prompt(
    name="movie-critic-chat",
    type="chat",
    prompt=[
      { "role": "system", "content": "You are an {{criticlevel}} movie critic" },
      { "role": "user", "content": "Do you like {{movie}}?" },
    ],
    labels=["production"],  # directly promote to production
    config={
        "model": "gpt-4o",
        "temperature": 0.7,
        "supported_languages": ["en", "fr"],
    },  # optionally, add configs (e.g. model parameters or model tools) or tags
)
```
<main id="notice" type='notice'>
    <h4>Note</h4>
    <p>If you already have a prompt with the same name, the prompt will be added as a new version.</p>
</main>

### JS/TS SDK

1. Install SDK.

```bash
npm i @langfuse/client
```

2. Add your Langfuse keys as environment variables.

```bash filename=".env"
LANGFUSE_SECRET_KEY = "sk-lf-...";
LANGFUSE_PUBLIC_KEY = "pk-lf-...";
LANGFUSE_BASE_URL = "xxx";
```

```ts
import { LangfuseClient } from "@langfuse/client";

const langfuse = new LangfuseClient();
```

```ts
import { LangfuseClient } from "@langfuse/client";

const langfuse = new LangfuseClient({
  secretKey: "sk-lf-...",
  publicKey: "pk-lf-...",
  baseUrl: "https://cloud.langfuse.com", // üá™üá∫ EU region
  // baseUrl: "https://us.cloud.langfuse.com", // üá∫üá∏ US region
});
```

3. Use the JS/TS SDK to create a new prompt or update an existing one.

```ts
// Create a text prompt
await langfuse.prompt.create({
  name: "movie-critic",
  type: "text",
  prompt: "As a {{criticlevel}} critic, do you like {{movie}}?",
  labels: ["production"], // directly promote to production
  config: {
    model: "gpt-4o",
    temperature: 0.7,
    supported_languages: ["en", "fr"],
  }, // optionally, add configs (e.g. model parameters or model tools) or tags
});

// Create a chat prompt
await langfuse.prompt.create({
  name: "movie-critic-chat",
  type: "chat",
  prompt: [
    { role: "system", content: "You are an {{criticlevel}} movie critic" },
    { role: "user", content: "Do you like {{movie}}?" },
  ],
  labels: ["production"], // directly promote to production
  config: {
    model: "gpt-4o",
    temperature: 0.7,
    supported_languages: ["en", "fr"],
  }, // optionally, add configs (e.g. model parameters or model tools) or tags
});
```

<main id="notice" type='explain'>
    <h4>Note</h4>
    <p>If you already have a prompt with the same name, the prompt will be added as a new version.</p>
</main>

### Use prompt

At runtime, you can fetch the latest production version from Langfuse. Learn more about versions/labels, see [version control](../30.Prompt%20Management//30.Features/00.Version%20control.md).

### Python SDK

```python
from langfuse import get_client

# Initialize Langfuse client
langfuse = get_client()
```

**Text prompt**

```python
# Get current `production` version of a text prompt
prompt = langfuse.get_prompt("movie-critic")

# Insert variables into prompt template
compiled_prompt = prompt.compile(criticlevel="expert", movie="Dune 2")
# -> "As an expert movie critic, do you like Dune 2?"
```

**Chat prompt**

```python
# Get current `production` version of a chat prompt
chat_prompt = langfuse.get_prompt("movie-critic-chat", type="chat") # type arg infers the prompt type (default is 'text')

# Insert variables into chat prompt template
compiled_chat_prompt = chat_prompt.compile(criticlevel="expert", movie="Dune 2")
# -> [{"role": "system", "content": "You are an expert movie critic"}, {"role": "user", "content": "Do you like Dune 2?"}]
```

**Optional parameters**

```python
# Get specific version
prompt = langfuse.get_prompt("movie-critic", version=1)

# Get specific label
prompt = langfuse.get_prompt("movie-critic", label="staging")

# Get latest prompt version. The 'latest' label is automatically maintained by Langfuse.
prompt = langfuse.get_prompt("movie-critic", label="latest")
```

**Attributes**

```python
# Raw prompt including {{variables}}. For chat prompts, this is a list of chat messages.
prompt.prompt

# Config object
prompt.config
```
### JS/TS SDK

```ts
import { LangfuseClient } from "@langfuse/client";

// Iniitialize the Langfuse client
const langfuse = new LangfuseClient();
```

**Text prompt**

```ts
// Get current `production` version
const prompt = await langfuse.prompt.get("movie-critic");

// Insert variables into prompt template
const compiledPrompt = prompt.compile({
  criticlevel: "expert",
  movie: "Dune 2",
});
// -> "As an expert movie critic, do you like Dune 2?"
```

**Chat prompt**

```ts
// Get current `production` version of a chat prompt
const chatPrompt = await langfuse.prompt.get("movie-critic-chat", {
  type: "chat",
}); // type option infers the prompt type (default is 'text')

// Insert variables into chat prompt template
const compiledChatPrompt = chatPrompt.compile({
  criticlevel: "expert",
  movie: "Dune 2",
});
// -> [{"role": "system", "content": "You are an expert movie critic"}, {"role": "user", "content": "Do you like Dune 2?"}]
```

**Optional parameters**

```ts
// Get specific version of a prompt (here version 1)
const prompt = await langfuse.prompt.get("movie-critic", {
  version: 1
});

// Get specific label
const prompt = await langfuse.prompt.get("movie-critic", {
  label: "staging",
});

// Get latest prompt version. The 'latest' label is automatically maintained by Langfuse.
const prompt = await langfuse.prompt.get("movie-critic", {
  label: "latest",
});
```

**Attributes**

```ts
// Raw prompt including {{variables}}. For chat prompts, this is a list of chat messages.
prompt.prompt;

// Config object
prompt.config;
```

### Link with Langfuse Tracing (optional)

You can link the prompt to the LLM `generation` span that used the prompt. This linkage enables tracking of metrics by prompt version and name directly in the Langfuse UI and see which prompt performed best.

### Python SDK

**Decorators**

```python
from langfuse import observe, get_client

langfuse = get_client()

@observe(as_type="generation")
def nested_generation():
    prompt = langfuse.get_prompt("movie-critic")

    langfuse.update_current_generation(
        prompt=prompt,
    )

@observe()
def main():
  nested_generation()

main()
```

**Context Managers**

```python
from langfuse import get_client

langfuse = get_client()

prompt = langfuse.get_prompt("movie-critic")

with langfuse.start_as_current_observation(
    as_type="generation",
    name="movie-generation",
    model="gpt-4o",
    prompt=prompt
) as generation:
    # Your LLM call here
    generation.update(output="LLM response")
```

### JS/TS SDK

**Manual observations**

```ts
import { LangfuseClient } from "@langfuse/client";
import { startObservation } from "@langfuse/tracing";

const prompt = new LangfuseClient().prompt.get("my-prompt");

startObservation(
  "llm",
  {
    prompt,
  },
  { asType: "generation" },
);
```

**Context manager**

```ts
import { LangfuseClient } from "@langfuse/client";
import { updateActiveObservation } from "@langfuse/tracing";

const langfuse = new LangfuseClient();

startActiveObservation(
  "llm",
  async (generation) => {
    const prompt = langfuse.prompt.get("my-prompt");
    generation.update({ prompt });
  },
  { asType: "generation" },
);
```

**Observe wrapper**

```ts
import { LangfuseClient } from "@langfuse/client";
import { observe } from "@langfuse/tracing";

const langfuse = new LangfuseClient();

const callLLM = async (input: string) => {
  const prompt = langfuse.prompt.get("my-prompt");

  updateActiveObservation({ prompt }, { asType: "generation" });

  return await invokeLLM(input);
};

export const observedCallLLM = observe(callLLM);
```
