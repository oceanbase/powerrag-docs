# LLM-as-a-Judge

LLM-as-a-judge is a technique to evaluate the quality of LLM applications by using an LLM as a judge. The LLM is given a trace or a dataset entry and asked to score and reason about the output. The resulting [`scores`](../10.Evaluation%20methods/10.Scores%20data%20model.md) include chain-of-thought reasoning as a `comment`.

## Why use LLM-as-a-judge?

- **Scalable & cost‑effective:** Judge thousands of outputs quickly and cheaply versus human panels.
- **Human‑like judgments:** Captures nuance (helpfulness, safety, coherence) better than simple metrics, especially when rubric‑guided.
- **Repeatable comparisons:** With a fixed rubric, you can rerun the same prompts to get consistent scores and short rationales.

## Set up step-by-step

### Set the default model

This step requires an LLM Connection to be set up. Please see [Configure an LLM Connection](../30.Configure%20an%20LLM%20connection.md) for more information.

1. In the left-side navigation pane, click **LLM-as-a-Judge**。
2. Click **Set up evaluator**.
3. Click ![](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/powerrag/langfuse/icon6.png) in the upper right corner to select the default model.

The default model is used by every managed evaluator; custom templates may override it.

Notes:

- **Setup**: This default model needs to be set up once, though it can be changed at any point if needed.
- **Change**: Existing evaluators keep evaluating with the new model—historic results stay preserved.
- **Structured Output Support**: It's crucial that the chosen default model supports structured output. This is essential for our system to correctly interpret the evaluation results from the LLM judge.

### Pick an evaluator

![](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/powerrag/langfuse/llm-judge1-en.png)

Now we select an evaluator. There are two main ways:

+ Built-in evaluators:

Langfuse ships a growing catalog of evaluators built and maintained by us. Each evaluator captures best-practice evaluation prompts for a specific quality dimension—e.g. _Hallucination_, _Context-Relevance_, _Toxicity_, _Helpfulness_.

- **Ready to use**: no prompt writing required.
- **Continuously expanded**: by adding OSS partner-maintained evaluators and more evaluator types in the future (e.g. regex-based).

+ Custom evaluators:

When the library doesn't fit your specific needs, add your own:

1. Draft an evaluation prompt with `{{variables}}` placeholders (`input`, `output`, `ground_truth` …).
2. Optional: Customize the **score** (0-1) and **reasoning** prompts to guide the LLM in scoring.
3. Optional: Pin a custom dedicated model for this evaluator. If no custom model is specified, it will use the default evaluation model (see Section 2).
4. Save → the evaluator can now be reused across your project.

### Choose which data to evaluate

With your evaluator and model selected, you now specify which data to run the evaluations on. You can choose between running on **production tracing data** or **Datasets during Dataset Runs**.

#### Production tracing data

Evaluating live production traffic allows you to monitor the performance of your LLM application in real-time.

- **Scope**: Choose whether to run on _new_ traces only and/or _existing_ traces once (for backfilling). When in doubt, we recommend running on _new_ traces.
- **Filter**: Narrow down the evaluation to a specific subset of data you're interested in. You can filter by trace name, tags, `userId` and may more. Combine filters freely.
- **Preview**: Langfuse shows a sample of traces from the last 24 hours that match your current filters, allowing you to sanity-check your selection.
- **Sampling**: To manage costs and evaluation throughput, you can configure the evaluator to run on a percentage (e.g., 5%) of the matched traces.

![](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/powerrag/langfuse/llm-judge2-en.png)

#### Datasets during dataset runs

LLM-as-a-Judge evaluators can score the results of your Experiments.

**[Experiments via UI](../20.Experiments/30.Experiments%20via%20UI.md)**: When running Experiments via UI, you can simply select which evaluators you want to run. These selected evaluators will then automatically execute on the data generated by your next run.

**[Experiments via SDK](../20.Experiments/20.Experiments%20via%20SDK.md)**: You can configure evaluators directly in the code by using the Experiment Runner SDK.

### Map variables & preview evaluation prompt

You now need to teach Langfuse _which properties_ of your trace or dataset item represent the actual data to populate these variables for a sensible evaluation. For instance, you might map your system's logged trace input to the prompt's `{{input}}` variable, and the LLM response ie trace output to the prompt's `{{output}}` variable. This mapping is crucial for ensuring the evaluation is sensible and relevant.

#### Production tracing data

- **Prompt Preview**: As you configure the mapping, Langfuse shows a **live preview of the evaluation prompt populated with actual data**. This preview uses historical traces from the last 24 hours that matched your filters (from Step 3). You can navigate through several example traces to see how their respective data fills the prompt, helping you build confidence that the mapping is correct.
- **JSONPath**: If the data is nested (e.g., within a JSON object), you can use a JSONPath expression (like `$.choices[0].message.content`) to precisely locate it.

![](https://langfuse.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fevaluator-mapping.61f9d25c.png&w=3840&q=75)

#### Datasets during dataset runs

- **Suggested mappings**: The system will often be able to autocomplete common mappings based on typical field names in datasets. For example, if you're evaluating for correctness, and your prompt includes `{{input}}`, `{{output}}`, and `{{ground_truth}}` variables, we would likely suggest mapping these to the trace input, trace output, and the dataset item's expected_output respectively.
- **Edit mappings**: You can easily edit these suggestions if your dataset schema differs. You can map any properties of your dataset item (e.g., `input`, `expected_output`). Further, as dataset runs create traces under the hood, using the trace input/output as the evaluation input/output is a common pattern. Think of the trace output as your experiment run's output.

## Monitor & iterate

As our system evaluates your data it writes the results as [scores](10.Scores%20data%20model.md). You can then:

- **View Logs**: Check detailed logs for each evaluation, including status, execution trace IDs, and retry errors. Click the execution trace ID to view the complete LLM interaction for that specific evaluation run.
- **Use Dashboards**: Aggregate scores over time, filter by version or environment, and track the performance of your LLM application.
- **Take Actions**: Pause, resume, or delete an evaluator.

![](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/powerrag/langfuse/llm-judge4-en.png)

## Trace evaluator executions

Every LLM-as-a-Judge evaluator execution now creates a full trace, giving you complete visibility into the evaluation process. This allows you to:

- **Debug prompt issues**: See exactly what prompt was sent to the judge LLM
- **Inspect model responses**: View the complete response including reasoning and structured outputs
- **Monitor token usage**: Track costs and performance for each evaluator execution
- **Trace evaluation history**: Navigate from any score back to its source LLM interaction

### How to access execution traces

There are four ways to navigate to an evaluator execution trace:

1. **Score tooltip in trace view**: For LLM-as-a-Judge scores, hover over any score badge in the trace detail view and click "View execution trace"
2. **Tracing table**: Filter the environment to `langfuse-llm-as-a-judge` to view all evaluator execution traces
3. **Scores table**: Enable the "Execution Trace" column in the scores table to see execution trace IDs for all evaluator executions
4. **Evaluator logs table**: View execution trace IDs in the evaluator logs for detailed execution history and status

The execution trace shows the exact prompt sent to the judge, the model's response, token usage, latency, and any errors that occurred during execution.

### Understanding execution statuses

- **Completed**: Evaluation finished successfully
- **Error**: Evaluation failed (click execution trace ID for details)
- **Delayed**: Evaluation hit rate limits by the LLM provider and is being retried with exponential backoff
- **Pending**: Evaluation is queued and waiting to run
