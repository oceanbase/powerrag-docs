# 通过 SDK 跑测

通过 SDK 进行的跑测可用于以编程方式循环运行您的应用或提示词，使其遍历数据集，并可选择将评估方法应用于结果。您可以使用 Langfuse 上托管的数据集或本地数据集作为跑测的基础。

## 为什么通过 SDK 进行跑测？
+ 完全灵活地使用您自己的应用逻辑。
+ 使用自定义评分函数来评估单个项目和完整运行的输出。
+ 在同一数据集上并行运行多个跑测。
+ 易于与您现有的评估基础设施集成。

## 运行跑测的 SDK
Python 提供了用于在数据集上运行跑测的高级抽象。数据集可以是本地的，也可以托管在 Langfuse 上。建议使用 SDK 在数据集上运行跑测。

跑测运行器自动处理：

+ 可配置限制的任务**并发执行。**
+ **自动跟踪**所有执行，以提高可观测性。
+ 使用项目级和运行级评估器**进行灵活评估。**
+ **错误隔离，**因此个别故障不会停止跑测。
+ **数据集集成**，便于比较和跟踪。

SDK 支持 Langfuse 上托管的数据集和本地托管的数据集。如果您使用 Langfuse 上托管的数据集进行跑测，SDK 将自动为您创建一个数据集跑测，您可以在 Langfuse 控制台中检查和比较该数据集。对于不在 Langfuse 上托管的本地数据集，Langfuse 仅支持跟踪和评分（如果使用了评估）。

### 基本用法
从最简单的跑测开始，在本地数据上测试你的任务函数。如果你已经在 Langfuse 中拥有数据集，参见 [与 Langfuse 数据集一起使用](#usage-with-langfuse-datasets) 章节。

在本地数据上运行跑测时，Langfuse 中只会创建跟踪，不会生成任何数据集跑测。每次任务执行都会创建一个单独的跟踪，以便于观测和调试。

#### 使用 Python SDK
```python
from langfuse import get_client
from langfuse.openai import OpenAI

# 初始化客户端
langfuse = get_client()

# 定义任务函数
def my_task(*, item, **kwargs):
    question = item["input"]
    response = OpenAI().chat.completions.create(
        model="gpt-4.1", messages=[{"role": "user", "content": question}]
    )

    return response.choices[0].message.content


# 在本地数据上运行跑测
local_data = [
    {"input": "What is the capital of France?"},
    {"input": "What is the capital of Germany?"},
]

result = langfuse.run_experiment(
    name="Geography Quiz",
    description="Testing basic functionality",
    data=local_data,
    task=my_task,
)

# 使用 format 方法显示结果
print(result.format())
```

#### 使用 JS/TS SDK
确保正确设置`OpenTelemetry`，以便将跟踪信息传送到 Langfuse。执行结束时务必刷新 span 处理器，以确保所有跟踪信息均已发送。

```python
import { OpenAI } from "openai";
import { NodeSDK } from "@opentelemetry/sdk-node";
 
import {
  LangfuseClient,
  ExperimentTask,
  ExperimentItem,
} from "@langfuse/client";
import { observeOpenAI } from "@langfuse/openai";
import { LangfuseSpanProcessor } from "@langfuse/otel";
 
// 初始化 OpenTelemetry
const otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });
otelSdk.start();
 
// 初始化客户端
const langfuse = new LangfuseClient();
 
// 在本地数据上运行跑测
const localData: ExperimentItem[] = [
  { input: "What is the capital of France?", expectedOutput: "Paris" },
  { input: "What is the capital of Germany?", expectedOutput: "Berlin" },
];
 
// 定义任务函数
const myTask: ExperimentTask = async (item) => {
  const question = item.input;
 
  const response = await observeOpenAI(new OpenAI()).chat.completions.create({
    model: "gpt-4.1",
    messages: [
      {
        role: "user",
        content: question,
      },
    ],
  });
 
  return response;
};
 
// 运行跑测
const result = await langfuse.experiment.run({
  name: "Geography Quiz",
  description: "Testing basic functionality",
  data: localData,
  task: myTask,
});
 
// 打印已格式化的结果
console.log(await result.format());
 
// 重要：关闭 OTEL SDK 来发送跟踪
await otelSdk.shutdown();
```



### 与 Langfuse 数据集一起使用
直接跑测存储在 Langfuse 中的数据集，以进行自动跟踪和比较。

使用 Langfuse 数据集时，数据集跑测会自动在 Langfuse 中创建，并可在控制台中进行比较。这可以跟踪跑测随时间的变化，并比较同一数据集上不同方法的运行情况。

#### 使用 Python SDK
```python
# 获取数据集
dataset = langfuse.get_dataset("my-evaluation-dataset")
 
# 直接在数据集上运行跑测
result = dataset.run_experiment(
    name="Production Model Test",
    description="Monthly evaluation of our production model",
    task=my_task # 请参见上面的任务定义
)
 
# 使用 format 方法显示结果
print(result.format())
```



#### 使用 JS/TS SDK
确保正确设置`OpenTelemetry`，以便将跟踪信息传送到 Langfuse。执行结束时务必刷新 span 处理器，以确保所有跟踪信息均已发送。

```python
// 获取数据集
const dataset = await langfuse.dataset.get("my-evaluation-dataset");
 
// 直接在数据集上运行跑测
const result = await dataset.runExperiment({
  name: "Production Model Test",
  description: "Monthly evaluation of our production model",
  task: myTask, // 请参见上面的任务定义
});
 
// 使用 format 方法显示结果
console.log(await result.format());
 
// 重要：关闭 OpenTelemetry 来确保发送跟踪到 Langfuse
await otelSdk.shutdown();
```

### 高级功能
使用评估器和高级配置选项增强您的跑测。

#### 配置评估器
评估器在项目级别评估任务输出的质量。它们接收每个项目的输入、元数据、输出和预期输出，并返回评估指标，这些指标在 Langfuse 的跟踪上以评分的形式报告。

##### 使用 Python SDK
```python
from langfuse import Evaluation
 
# 定义评估器函数
def accuracy_evaluator(*, input, output, expected_output, metadata, **kwargs):
    if expected_output and expected_output.lower() in output.lower():
        return Evaluation(name="accuracy", value=1.0, comment="Correct answer found")
 
    return Evaluation(name="accuracy", value=0.0, comment="Incorrect answer")
 
def length_evaluator(*, input, output, **kwargs):
    return Evaluation(name="response_length", value=len(output), comment=f"Response has {len(output)} characters")
 
# 使用多个评估器
result = langfuse.run_experiment(
    name="Multi-metric Evaluation",
    data=test_data,
    task=my_task,
    evaluators=[accuracy_evaluator, length_evaluator]
)
 
print(result.format())
```

##### 使用 JS/TS SDK
```python
// 定义评估器函数
const accuracyEvaluator = async ({ input, output, expectedOutput }) => {
  if (
    expectedOutput &&
    output.toLowerCase().includes(expectedOutput.toLowerCase())
  ) {
    return {
      name: "accuracy",
      value: 1.0,
      comment: "Correct answer found",
    };
  }
  return {
    name: "accuracy",
    value: 0.0,
    comment: "Incorrect answer",
  };
};
 
const lengthEvaluator = async ({ input, output }) => {
  return {
    name: "response_length",
    value: output.length,
    comment: `Response has ${output.length} characters`,
  };
};
 
// 使用多个评估器
const result = await langfuse.experiment.run({
  name: "Multi-metric Evaluation",
  data: testData,
  task: myTask,
  evaluators: [accuracyEvaluator, lengthEvaluator],
});
 
console.log(await result.format());
```

#### 跑测级评估器
跑测级评估器评估完整的跑测结果并计算聚合指标。在 Langfuse 数据集上跑测时，这些评分会附加到完整的数据集跑测中，以跟踪整体跑测性能。

##### 使用 Python SDK
```python
from langfuse import Evaluation
 
def average_accuracy(*, item_results, **kwargs):
    """计算所有项目的平均准确性"""
    accuracies = [
        eval.value for result in item_results
        for eval in result.evaluations
        if eval.name == "accuracy"
    ]
 
    if not accuracies:
        return Evaluation(name="avg_accuracy", value=None)
 
    avg = sum(accuracies) / len(accuracies)
 
    return Evaluation(name="avg_accuracy", value=avg, comment=f"Average accuracy: {avg:.2%}")
 
result = langfuse.run_experiment(
    name="Comprehensive Analysis",
    data=test_data,
    task=my_task,
    evaluators=[accuracy_evaluator],
    run_evaluators=[average_accuracy]
)
 
print(result.format())
```

##### 使用 JS/TS SDK
```python
const averageAccuracy = async ({ itemResults }) => {
  // 计算所有项目的平均准确性
  const accuracies = itemResults
    .flatMap((result) => result.evaluations)
    .filter((evaluation) => evaluation.name === "accuracy")
    .map((evaluation) => evaluation.value as number);
 
  if (accuracies.length === 0) {
    return { name: "avg_accuracy", value: null };
  }
 
  const avg = accuracies.reduce((sum, val) => sum + val, 0) / accuracies.length;
 
  return {
    name: "avg_accuracy",
    value: avg,
    comment: `Average accuracy: ${(avg * 100).toFixed(1)}%`,
  };
};
 
const result = await langfuse.experiment.run({
  name: "Comprehensive Analysis",
  data: testData,
  task: myTask,
  evaluators: [accuracyEvaluator],
  runEvaluators: [averageAccuracy],
});
 
console.log(await result.format());
```

#### 异步任务和评估器
任务函数和评估器都可以是异步的。

##### 使用 Python SDK
```python
import asyncio
from langfuse.openai import AsyncOpenAI
 
async def async_llm_task(*, item, **kwargs):
    """使用 OpenAI 的异步任务"""
    client = AsyncOpenAI()
    response = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": item["input"]}]
    )
 
    return response.choices[0].message.content
 
# 与异步函数无缝协作
result = langfuse.run_experiment(
    name="Async Experiment",
    data=test_data,
    task=async_llm_task,
    max_concurrency=5  # 控制并发 API 调用
)
 
print(result.format())
```

##### 使用 JS/TS SDK
```python
import OpenAI from "openai";
 
const asyncLlmTask = async (item) => {
  // 使用 OpenAI 的异步任务
  const client = new OpenAI();
  const response = await client.chat.completions.create({
    model: "gpt-4",
    messages: [{ role: "user", content: item.input }],
  });
 
  return response.choices[0].message.content;
};
 
// 与异步函数无缝协作
const result = await langfuse.experiment.run({
  name: "Async Experiment",
  data: testData,
  task: asyncLlmTask,
  maxConcurrency: 5, // 控制并发 API 调用
});
 
console.log(await result.format());
```

#### 配置选项
使用各种配置选项自定义跑测行为。

##### 使用 Python SDK
```python
result = langfuse.run_experiment(
    name="Configurable Experiment",
    run_name="Custom Run Name", # 若使用数据集，传入数据集跑测名称
    description="Experiment with custom configuration",
    data=test_data,
    task=my_task,
    evaluators=[accuracy_evaluator],
    run_evaluators=[average_accuracy],
    max_concurrency=10,  # 最大并发执行数
    metadata={  # 附加到所有跟踪
        "model": "gpt-4",
        "temperature": 0.7,
        "version": "v1.2.0"
    }
)
 
print(result.format())
```

##### 使用 JS/TS SDK
```python
const result = await langfuse.experiment.run({
  name: "Configurable Experiment",
  runName: "Custom Run Name", // 若使用数据集，传入数据集跑测名称
  description: "Experiment with custom configuration",
  data: testData,
  task: myTask,
  evaluators: [accuracyEvaluator],
  runEvaluators: [averageAccuracy],
  maxConcurrency: 10, // 最大并发执行数
  metadata: {
    // 附加到所有跟踪
    model: "gpt-4",
    temperature: 0.7,
    version: "v1.2.0",
  },
});
 
console.log(await result.format());
```

#### 在 CI 环境中测试
将跑测运行器与 Pytest 和 Vitest 等测试框架集成，以便在您的 CI 管道中运行自动化评估。使用评估器创建断言，这些断言可根据评估结果判定测试失败。

这些示例展示了如何使用跑测运行器的评估结果在 CI 流水线中创建有意义的测试断言。当准确率低于可接受的阈值时，测试可能会失败，从而确保模型质量标准能够自动维持。

##### 使用 Python SDK
```python
# test_geography_experiment.py
import pytest
from langfuse import get_client, Evaluation
from langfuse.openai import OpenAI
 
# 欧洲首都测试数据
test_data = [
    {"input": "What is the capital of France?", "expected_output": "Paris"},
    {"input": "What is the capital of Germany?", "expected_output": "Berlin"},
    {"input": "What is the capital of Spain?", "expected_output": "Madrid"},
]
 
def geography_task(*, item, **kwargs):
    """任务功能为回答地理问题"""
    question = item["input"]
    response = OpenAI().chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": question}]
    )
    return response.choices[0].message.content
 
def accuracy_evaluator(*, input, output, expected_output, **kwargs):
    """评估器检查输出中是否有期望的回答"""
    if expected_output and expected_output.lower() in output.lower():
        return Evaluation(name="accuracy", value=1.0)
 
    return Evaluation(name="accuracy", value=0.0)
 
def average_accuracy_evaluator(*, item_results, **kwargs):
    """运行评估器计算所有项目的平均准确性"""
    accuracies = [
        eval.value for result in item_results
        for eval in result.evaluations if eval.name == "accuracy"
    ]
 
    if not accuracies:
        return Evaluation(name="avg_accuracy", value=None)
 
    avg = sum(accuracies) / len(accuracies)
 
    return Evaluation(name="avg_accuracy", value=avg, comment=f"Average accuracy: {avg:.2%}")
 
@pytest.fixture
def langfuse_client():
    """初始化用于测试的客户端"""
    return get_client()
 
def test_geography_accuracy_passes(langfuse_client):
    """准确性高于阈值的测试已通过"""
    result = langfuse_client.run_experiment(
        name="Geography Test - Should Pass",
        data=test_data,
        task=geography_task,
        evaluators=[accuracy_evaluator],
        run_evaluators=[average_accuracy_evaluator]
    )
 
    # 直接访问评估器运行结果
    avg_accuracy = next(
        eval.value for eval in result.run_evaluations
        if eval.name == "avg_accuracy"
    )
 
    # 断言最小准确性阈值
    assert avg_accuracy >= 0.8, f"Average accuracy {avg_accuracy:.2f} below threshold 0.8"
 
def test_geography_accuracy_fails(langfuse_client):
    """示例测试演示失败情况"""
    # 使用更弱的模型或更难的问题演示测试失败
    def failing_task(*, item, **kwargs):
        # 模拟一个给出错误答案的任务
        return "I don't know"
 
    result = langfuse_client.run_experiment(
        name="Geography Test - Should Fail",
        data=test_data,
        task=failing_task,
        evaluators=[accuracy_evaluator],
        run_evaluators=[average_accuracy_evaluator]
    )
 
    # 直接访问评估器运行结果
    avg_accuracy = next(
        eval.value for eval in result.run_evaluations
        if eval.name == "avg_accuracy"
    )
 
    # 本测试失败，因为任务给出了错误答案
    with pytest.raises(AssertionError):
        assert avg_accuracy >= 0.8, f"Expected test to fail with low accuracy: {avg_accuracy:.2f}"
```



##### 使用 JS/TS SDK
```python
// test/geography-experiment.test.ts
import { describe, it, expect, beforeAll, afterAll } from "vitest";
import { OpenAI } from "openai";
import { NodeSDK } from "@opentelemetry/sdk-node";
import { LangfuseClient, ExperimentItem } from "@langfuse/client";
import { observeOpenAI } from "@langfuse/openai";
import { LangfuseSpanProcessor } from "@langfuse/otel";
 
// 欧洲首都测试数据
const testData: ExperimentItem[] = [
  { input: "What is the capital of France?", expectedOutput: "Paris" },
  { input: "What is the capital of Germany?", expectedOutput: "Berlin" },
  { input: "What is the capital of Spain?", expectedOutput: "Madrid" },
];
 
let otelSdk: NodeSDK;
let langfuse: LangfuseClient;
 
beforeAll(async () => {
  // 初始化 OpenTelemetry
  otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });
  otelSdk.start();
 
  // 初始化客户端
  langfuse = new LangfuseClient();
});
 
afterAll(async () => {
  // 清理 shutdown
  await otelSdk.shutdown();
});
 
const geographyTask = async (item: ExperimentItem) => {
  const question = item.input;
  const response = await observeOpenAI(new OpenAI()).chat.completions.create({
    model: "gpt-4",
    messages: [{ role: "user", content: question }],
  });
 
  return response.choices[0].message.content;
};
 
const accuracyEvaluator = async ({ input, output, expectedOutput }) => {
  if (
    expectedOutput &&
    output.toLowerCase().includes(expectedOutput.toLowerCase())
  ) {
    return { name: "accuracy", value: 1 };
  }
  return { name: "accuracy", value: 0 };
};
 
const averageAccuracyEvaluator = async ({ itemResults }) => {
  // 计算所有项目的平均准确性
  const accuracies = itemResults
    .flatMap((result) => result.evaluations)
    .filter((evaluation) => evaluation.name === "accuracy")
    .map((evaluation) => evaluation.value as number);
 
  if (accuracies.length === 0) {
    return { name: "avg_accuracy", value: null };
  }
 
  const avg = accuracies.reduce((sum, val) => sum + val, 0) / accuracies.length;
  return {
    name: "avg_accuracy",
    value: avg,
    comment: `Average accuracy: ${(avg * 100).toFixed(1)}%`,
  };
};
 
describe("Geography Experiment Tests", () => {
  it("should pass when accuracy is above threshold", async () => {
    const result = await langfuse.experiment.run({
      name: "Geography Test - Should Pass",
      data: testData,
      task: geographyTask,
      evaluators: [accuracyEvaluator],
      runEvaluators: [averageAccuracyEvaluator],
    });
 
    // 直接访问评估器运行结果
    const avgAccuracy = result.runEvaluations.find(
      (eval) => eval.name === "avg_accuracy"
    )?.value as number;
 
    // 断言最小准确性阈值
    expect(avgAccuracy).toBeGreaterThanOrEqual(0.8);
  }, 30_000); // API 调用 30 秒超时
 
  it("should fail when accuracy is below threshold", async () => {
    // 任务给出错误答案来演示测试失败
    const failingTask = async (item: ExperimentItem) => {
      return "I don't know";
    };
 
    const result = await langfuse.experiment.run({
      name: "Geography Test - Should Fail",
      data: testData,
      task: failingTask,
      evaluators: [accuracyEvaluator],
      runEvaluators: [averageAccuracyEvaluator],
    });
 
    // 直接访问评估器运行结果
    const avgAccuracy = result.runEvaluations.find(
      (eval) => eval.name === "avg_accuracy"
    )?.value as number;
 
    // 本测试失败，因为任务给出了错误答案
    expect(() => {
      expect(avgAccuracy).toBeGreaterThanOrEqual(0.8);
    }).toThrow();
  }, 30_000);
});
```

## 低级 SDK 方法
如果您需要对数据集跑测进行更多控制，则可以使用低级 SDK 方法来循环遍历数据集项并执行应用逻辑。

### 加载数据集
使用 Python SDK 加载数据集。

#### 使用 Python SDK
```python
from langfuse import get_client
 
dataset = get_client().get_dataset("<dataset_name>")
```

#### 使用 JS/TS SDK
```python
import { LangfuseClient } from "@langfuse/client";
 
const langfuse = new LangfuseClient();
 
const dataset = await langfuse.dataset.get("<dataset_name>");
```

### 装配您的应用
首先，我们创建应用运行器辅助函数。下一步，每个数据集项都会调用此函数。如果您使用 Langfuse 来实现生产可观测性，则无需更改应用代码。

<main id="notice" type='explain'>
    <h4>说明</h4>
    <p>对于数据集跑测，重要的是您的应用为每次跑测创建 Langfuse 跟踪，以便它们可以链接到数据集项。</p>
</main>

#### 使用 Python SDK
假设您的 LLM 应用已经装配了 Langfuse：

```python
from langfuse import get_client, observe
from langfuse.openai import OpenAI
 
@observe
def my_llm_function(question: str):
    response = OpenAI().chat.completions.create(
        model="gpt-4o", messages=[{"role": "user", "content": question}]
    )
    output = response.choices[0].message.content
 
    # 更新跟踪输入/输出
    get_client().update_current_trace(input=question, output=output)
 
    return output
```

#### 使用 JS/TS SDK
请确保已设置 JS/TS SDK 以用于应用的跟踪。如果您使用 Langfuse 进行[应用观测](/20.应用观测/00.概述.md)，则设置相同。

```tsx
import { OpenAI } from "openai"

import { LangfuseClient } from "@langfuse/client";
import { startActiveObservation } from "@langfuse/tracing";
import { observeOpenAI } from "@langfuse/openai";

const myLLMApplication = async (input: string) => {
  return startActiveObservation("my-llm-application", async (span) => {
    const output = await observeOpenAI(new OpenAI()).chat.completions.create({
      model: "gpt-4o",
      messages: [{ role: "user", content: input }],
    });

    span.update({ input, output: output.choices[0].message.content });

    // 返回 span 和输出的引用
    return [span, output] as const;
  }
};
```

### 在数据集上跑测
在数据集上跑测时，需要测试的应用会跑测数据集中的每个项目。然后，跟踪会关联到数据集项目。这允许您比较同一应用在同一数据集上的不同跑测结果。每个跑测都以`run_name`标识。

#### 使用 Python SDK
您可以为每个数据集项目执行该 LLM 应用来创建数据集跑测：

```python
from langfuse import get_client
from .app import my_llm_application
 
# 加载数据集
dataset = get_client().get_dataset("<dataset_name>")
 
# 循环遍历数据集项
for item in dataset.items:
    # 使用 item.run() 上下文管理器来自动连接跟踪
    with item.run(
        run_name="<run_name>",
        run_description="My first run",
        run_metadata={"model": "llama3"},
    ) as root_span:
        # 对数据集项输入执行 LLM 应用
        output = my_llm_application.run(item.input)
 
        # 可选：添加在跑测运行器中计算的评分，例如 JSON 相等性检查。
        root_span.score_trace(
            name="<example_eval>",
            value=my_eval_fn(item.input, output, item.expected_output),
            comment="This is a comment",  # 可选：添加推理会有帮助
        )
 
# 刷新客户端，以确保在跑测结束时将所有评分数据发送到服务器。
get_client().flush()
```

#### 使用 JS/TS SDK
```tsx
import { LangfuseClient } from "@langfuse/client";
 
const langfuse = new LangfuseClient();
 
for (const item of dataset.items) {
  // 执行应用功能并获取对象（trace/span/generation/event，和其他观测类型）
  // 将返回用于评估跑测的输出
  // 还可以关联使用 id
  const [span, output] = await myLlmApplication.run(item.input);
 
  // 将跟踪关联到数据集项目，并给予 run_name
  await item.link(span, "<run_name>", {
    description: "My first run", // optional run description
    metadata: { model: "llama3" }, // optional run metadata
  });
 
  // 可选：添加评分
  langfuse.score.trace(span, {
    name: "<score_name>",
    value: myEvalFunction(item.input, output, item.expectedOutput),
    comment: "This is a comment", // 可选：添加推理会有帮助
  });
}
 
// 刷新客户端，以确保在跑测结束时将所有评分数据发送到服务器。
await langfuse.flush();
```



### 比较数据集跑测
每次在数据集上运行跑测后，您都可以检查数据集跑测表中的汇总评分并并排比较结果。

1. 从导航栏选择 **数据集**。
2. 在跑测列表中，选择多次跑测。
3. 选择页面右上角 **操作** > **比较**，将选择的多次跑测进行并排比较。

## 可选：从控制台触发 SDK 跑测
通过 SDK 设置跑测时，允许从 Langfuse 控制台触发跑测运行会很有帮助。

您需要配置一个 Webhook 来接收来自 Langfuse 的触发请求。

### 配置 Webhook
1. 从导航栏选择 **数据集**。
2. 单击要设置远程跑测触发器的数据集，进入数据集详情页面。
3. 单击页面右上角 **新建数据集跑测** 打开设置页面。
4. 在弹窗中单击 **通过 SDK/API** 下方的![](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/powerrag/langfuse/icon8.png)图标。
5. 输入您的外部评估服务 URL，该服务将在跑测触发时接收 Webhook。输入将发送到 Webhook 的默认 config，包含 PowerRAG 和桥接器的连接参数。 
示例 config 如下：

```tsx
{
  "bridge": "ragflow",
  "powerrag_api_key": "app-kApqRotZlu0Q********",
  "powerrag_api_url": "http://6.13.***.***:2801/service-api/v1/chat-messages",
  "inputs": {
    "user": "langfuse-dataset-runner"
  },
  "ragflow_api_key": "ragflow-UyYTBiYzg2OTJjODE********",
  "ragflow_chat_id": "64bee43e92c611f09517********"
}
```

6. 单击 **更新** 完成配置。

### 触发跑测
配置完成后，团队成员可以从控制台触发远程跑测。Langfuse 会将数据集元数据（ID 和名称）以及任何自定义配置发送到您的 Webhook。

1. 从导航栏选择 **数据集**。
2. 单击要设置远程跑测触发器的数据集，进入数据集详情页面。
3. 单击页面右上角 **新建数据集跑测** 打开设置页面。
4. 在弹窗中单击 **通过 SDK/API** 下方的 **Run**。

**典型工作流程**：您的 Webhook 接收请求，从 Langfuse 获取数据集，针对数据集项目运行您的应用，评估结果，并将评分作为新的跑测导入 Langfuse。

