# 概述

评估（Evaluation）是开发和部署 LLM 应用的关键环节。通常，团队会根据用例和开发过程的阶段，使用多种不同的评估方法来评估其 AI 应用的性能。

## 为什么使用 LLM 评估？
LLM 评估对于提高语言模型的准确性和稳健性至关重要，最终将提升用户体验和对 AI 应用的信任。以下是主要优势：

+ **保证质量**：检测幻觉、事实错误和不一致的输出，以确保您的人工智能应用提供可靠的结果。
+ **性能监控**：衡量不同场景和边缘情况下的响应质量、相关性和用户满意度。
+ **持续改进**：通过结构化评估指标确定需要改进的领域，并跟踪改进情况。
+ **用户信任**：通过系统评估展示一致、高质量的输出，建立对人工智能应用的信心。
+ **缓解风险**：在潜在问题影响到生产用户之前发现它们，从而降低用户体验不佳或声誉受损的可能性。

## 线下评估和线上评估
### 线下评估
+ 在受控环境中评估应用。
+ 通常使用精选的测试数据集而不是实时用户查询。
+ 在开发过程中大量使用（可以作为持续集成/持续交付管道的一部分）来衡量改进/回归。
+ 可重复，并且由于您有基本事实，您可以获得清晰的准确性指标。

### 线上评估
+ 在真实的实际环境中评估应用，即在生产中实际使用时。
+ 使用跟踪成功率、用户满意度评分或实时流量的其他指标的评估方法。
+ 在线评估的优势在于它能够捕捉到你在实验室环境中可能无法预料到的事情。
+ 可以包括收集隐式和显式的用户反馈，并可能运行影子测试或 A/B 测试。

在实践中，成功的评估会将线上和线下评估相结合。许多团队采用循环式评估方法。这样，评估才能持续进行，并不断改进。

![](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/powerrag/langfuse/workflow.png)

## 核心概念
| **概念** | **描述** |
| --- | --- |
| 评分 | 评分（Score） 是灵活的数据对象，可用于存储任何评估指标并将其链接到 Langfuse 中的其他对象。 |
| 评估方法 | 评估方法是为其他对象赋分的函数或工具。 |
| 数据集 | 数据集是输入和可选的预期输出的集合，可以在跑测期间使用。 |
| 数据集跑测 | 用于通过 LLM 应用运行数据集，并可选择将评估方法应用于运行结果。 |


## 评估方法
评估方法是为其他对象赋分的函数或工具。Langfuse 使用评分来存储评估指标，旨在灵活地表示任何评估指标。

Langfuse 目前支持通过 [LLM 作为评判](../40.应用评估/10.评估方法/00.LLM%20作为评判（LLM-as-a-Judge）.md)方法自动评分。

## 数据集跑测
数据集跑测用于循环运行你的 LLM 应用的数据集（本地或托管在 Langfuse 上），并可选择将评估方法应用于运行结果。这让你能够有策略地评估你的应用，并在受控条件下并排比较不同输入、提示词、模型或其他参数的性能。

在 Langfuse 中，我们区分了 [通过 SDK 进行的跑测](../40.应用评估/20.跑测/20.通过%20SDK%20跑测.md) 和 [通过控制台进行的跑测](../40.应用评估/20.跑测/30.通过控制台跑测.md)。通过控制台进行的跑测依赖于 Langfuse 平台上的数据集、提示词以及可选的 LLM 作为评判评估器，因此可以直接在平台上触发和执行。通过 SDK 进行的跑测非常灵活，可以从任何外部系统触发。

为了获得最佳的跑测运行比较体验，我们建议在 Langfuse 平台上管理底层[数据集](../40.应用评估/20.跑测/10.数据集.md)。

