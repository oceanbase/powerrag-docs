# 通过控制台跑测

您可以在 Langfuse 中通过控制台跑测来测试来自[提示词管理](../../30.提示词管理/00.概述.md)或语言模型的不同提示词版本，并且并排比较结果。

或者，您可以使用 [LLM 作为评判评估器](../10.评估方法/00.LLM%20作为评判（LLM-as-a-Judge）.md)根据预期输出自动对响应进行评分，以便进一步在总体层面上分析结果。

## 为什么使用提示词跑测？
+ 快速测试不同的提示词版本或模型。
+ 使用数据集来测试不同的提示词版本和模型，从而构建提示词测试。
+ 通过提示词跑测快速迭代提示词。
+ 可选择使用 LLM 作为评判评估器根据数据集的预期输出对响应进行评分。
+ 在进行快速更改时通过跑测来防止回归。

## 前提条件
### 创建可用的提示词
创建您想要测试和评估的提示词。创建提示词，请参见 [提示词管理](../../30.提示词管理/10.快速开始.md)。

**提示词在以下情况下可用**：提示词包含与数据集中用于数据集跑测的数据集项目键匹配的变量。

请参阅下面的示例。

#### 示例：提示词变量和数据集项键映射
提示词：

```python
{{ documentation }}
 
Question: {{question}}
```

数据集项目：

```python
{
  "documentation": "Langfuse is an LLM Engineering Platform",
  "question": "What is Langfuse?"
}
```

在此示例中：

+ 提示词变量`{{documentation}}`映射到 JSON 键`"documentation"`。
+ 提示词变量`{{question}}`映射到 JSON 键`"question"`。
+ 数据集项目的输入 JSON 中必须同时存在这两个键，才能成功跑测。

#### 示例：聊天消息占位符映射
除了变量之外，您还可以将聊天消息提示词中的占位符映射到数据集项目键。当数据集项目还包含要使用的聊天消息历史记录等内容时，此功能非常有用。您的聊天提示词需要包含一个带有名称的占位符。占位符内的变量不会被解析。

聊天提示词： 占位符名称`message_history`

数据集项目：

```python
{
  "message_history": [
    {
      "role": "user",
      "content": "What is Langfuse?"
    },
    {
      "role": "assistant",
      "content": "Langfuse is a tool for tracking and analyzing the performance of language models."
    }
  ],
  "question": "What is Langfuse?"
}
```

在此示例中：

+ 聊天提示词占位符`message_history`映射到 JSON 键`"message_history"`。
+ 提示词变量`{{question}}`映射到不在占位符消息内的变量中的 JSON 键 `"question"`。
+ 数据集项目的输入 JSON 中必须同时存在这两个键，才能成功跑测。

### 创建可用的数据集
创建一个数据集，其中包含您想要用于即时跑测的输入和预期输出。创建数据集，请参见 [数据集](10.数据集.md)。

**同时满足以下条件时，数据集可用**：

+ 数据集项包含 JSON 对象作为输入
+ 这些对象的 JSON 键与您将使用的提示词的提示词变量匹配。

请参阅下面的示例。

#### 提示词变量和数据集项目键映射
提示词：

```python
{{ documentation }}
 
Question: {{question}}
```

数据集项目：

```python
{
  "documentation": "Langfuse is an LLM Engineering Platform",
  "question": "What is Langfuse?"
}
```

在此示例中：

+ 提示词变量`{{documentation}}`映射到 JSON 键`"documentation"`。
+ 提示词变量`{{question}}`映射到 JSON 键`"question"`。
+ 数据集项目的输入 JSON 中必须同时存在这两个键，才能成功跑测。

### 配置 LLM 连接
跑测前，您需要添加模型。如何添加模型，请参见 [配置 LLM 连接](../30.配置%20LLM%20连接.md)。

### 可选：设置 LLM 作为评判
您可以设置一个 LLM 作为评判评估器，根据预期结果对答案进行评分。请确保将 LLM 作为评判的目标设置为“跑测”，并筛选您想要使用的数据集。设置  LLM 作为评判，请参见 [LLM 作为评判](../10.评估方法/00.LLM%20作为评判（LLM-as-a-Judge）.md)。

## 通过控制台触发跑测（提示词跑测）
### 开始跑测
1. 从导航栏选择 **数据集**。
2. 单击要启动数据集跑测的数据集，进入数据集详情页面。
3. 单击页面右上角 **新建数据集跑测** 打开设置页面。
4. 在弹窗中单击 **通过用户界面** 下方的 **配置**。
5. 在弹窗中输入数据集跑测名称、描述。
6. 选择要使用的提示词及其版本、模型、数据集、评估器。
7. 单击 **开始** 触发数据集跑测。

您将被重定向到数据集跑测页面。跑测可能需要几秒钟或几分钟才能完成，具体取决于提示词的复杂程度和数据集的大小。

### 比较跑测结果
每次运行跑测后，您都可以检查数据集跑测列表中的汇总评分，并并排比较结果。

1. 从导航栏选择 **数据集**。
2. 单击数据集名称，进入数据集详情页面。
3. 在跑测列表中，选择多次跑测。
4. 选择页面右上角 **操作** > **比较**，将选择的多次跑测进行并排比较。



