# Quick start

This topic guides you through integrating traces in Langfuse, helping you quickly experience the observability workflow.

## Get API keys

Before starting, you need to obtain API keys to access Langfuse. If you have already obtained keys during Langfuse deployment according to [Access Langfuse and Create API Key](../10.Deployment%20and%20Initialization/20.Access%20Langfuse%20and%20Create%20API%20Key.md), you can skip this step.

1. Log in to the PowerRAG console, and click **Observability** in the left navigation bar to enter the Langfuse console.
2. In the left navigation bar, click **Settings**, then create new API keys on the API Keys page in project settings, including Public Key and Secret Key.


## Import trace data

PowerRAG provides multi-language SDK integration methods. You can integrate the execution process of model calls into the tracing system through Python or JavaScript/TypeScript SDKs.

### Use Python SDK

PowerRAG provides compatible support for Langfuse Python SDK, which can be used to trace any large model or Agent calls.

1. Install the SDK.

    ```python
    pip install langfuse
    ```

2. Configure environment variables. Create a `.env` file in the project root directory and add the following configuration items:

    ```python
    LANGFUSE_SECRET_KEY = "sk-lf-..."
    LANGFUSE_PUBLIC_KEY = "pk-lf-..."
    LANGFUSE_HOST = "https://xxx"
    ```

3. Create a Trace using one of the following methods:

    1. **Use @observe decorator**  
    
        @observe is the simplest instrumentation method, suitable for quick integration and function-level tracing. It can be applied to any function, automatically capturing function names, input parameters, return values, and execution time, and automatically ending the current span when the function returns.

        ```python
        from langfuse import observe, get_client
        
        @observe
        def my_function():
            return "Hello, world!"  # Input/output and duration automatically captured
        
        my_function()
        
        # Manually flush events in short-lived applications
        langfuse = get_client()
        langfuse.flush()
        ```

    2. **Use context managers**  
    
        Context managers are more flexible, suitable for tracing different task chunks in applications. This method is suitable for complex workflows and scenarios requiring hierarchical visualization. It automatically handles span start and end when entering and exiting contexts, and supports nested spans.

        ```python
        from langfuse import get_client
        
        langfuse = get_client()
        
        # Create a span
        with langfuse.start_as_current_span(name="process-request") as span:
            # Business logic
            span.update(output="Processing complete")
        
            # Nest a generation task (e.g., LLM call)
            with langfuse.start_as_current_generation(name="llm-response", model="gpt-3.5-turbo") as generation:
                generation.update(output="Generated response")
        
        # All spans automatically close when exiting context
        
        # Flush events
        langfuse.flush()
        ```

    3. **Manual observations**  
    
        If you need complete control over span lifecycle (e.g., async tasks or cross-thread call scenarios), you can use manual methods to create and end spans. This method provides the highest flexibility but requires developers to manage span closing logic themselves.

        ```python
        from langfuse import get_client
        
        langfuse = get_client()
        
        # Manually create span
        span = langfuse.start_span(name="user-request")
        
        # Execute business logic
        span.update(output="Request processed")
        
        # Create child span
        nested_span = span.start_span(name="nested-span")
        nested_span.update(output="Nested span output")
        
        # Manually end child span and parent span
        nested_span.end()
        span.end()
        
        # Flush events
        langfuse.flush()
        ```

### Use JS/TS SDK

PowerRAG also supports using JavaScript or TypeScript SDK to integrate LLM applications into the observability system.

1. Install the SDK.

    ```bash
    npm install @langfuse/tracing
    ```

2. Add credentials. Create a `.env` file in the project root directory and ensure environment variables are loaded using packages such as dotenv:

    ```bash
    LANGFUSE_SECRET_KEY = "sk-lf-..."
    LANGFUSE_PUBLIC_KEY = "pk-lf-..."
    LANGFUSE_BASE_URL = "https://xxx"
    ```

3. Initialize OpenTelemetry.

    1. Install OpenTelemetry Node SDK:

        ```bash
        npm install @opentelemetry/sdk-node
        ```

    2. Create an `instrumentation.ts` file to initialize OpenTelemetry and register LangfuseSpanProcessor:

        ```typescript
        import { NodeSDK } from "@opentelemetry/sdk-node";
        import { LangfuseSpanProcessor } from "@langfuse/otel"; 
        
        const sdk = new NodeSDK({
        spanProcessors: [new LangfuseSpanProcessor()],
        });
        
        sdk.start();
        ```

    3. Import it in the application entry file:

        ```typescript
        import "./instrumentation"; // Must be loaded before all other imports
        ```

4. Add tracing logic to the application.

        ```typescript
        import { startActiveObservation, startObservation } from "@langfuse/tracing";

        await startActiveObservation("user-request", async (span) => {
        span.update({
            input: { query: "What is the capital of France?" },
        });

        // Create child generation task
        const generation = startObservation(
            "llm-call",
            {
            model: "gpt-4",
            input: [{ role: "user", content: "What is the capital of France?" }],
            },
            { asType: "generation" },
        );

        // Model inference logic
        generation
            .update({
            output: { content: "The capital of France is Paris." },
            })
            .end();

        span.update({ output: "Successfully answered." });
        });
        ```

### View traces

After running your application, you can view the traces just generated in the Observability module of the PowerRAG console. The system displays the trace structure, execution time for each node, input/output content, and cost statistics in a graphical way, helping you quickly understand the execution details and performance of model calls.

