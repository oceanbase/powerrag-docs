# LLM 作为评判（LLM-as-a-Judge）

LLM 作为评判是一种以 LLM 为评判者来评估 LLM 应用质量的技术。LLM 会被给予一个跟踪（Trace）或数据集条目，并被要求对输出进行评分和推理。[评分结果](10.评分数据模型.md)中，会以注释的形式包含思维链推理过程。

## 为什么使用 LLM 作为评判？
+ **可扩展且经济高效**：与人工评审相比，可以快速且低价地评判数千个输出。
+ **类似人类的判断**：比简单的指标更好地捕捉细微差别（有用性、安全性、连贯性），尤其是在规则指导下。
+ **可重复的比较**：使用固定的评分标准，您可以重新运行相同的提示词以获得一致的分数和简短的理由。

## 设置步骤
### 设置默认模型
设置默认模型前，请添加需要的模型。如何添加模型，请参见 [配置 LLM 连接](../30.配置%20LLM%20连接.md)。

1. 在导航栏选择 **LLM 作为评判**。
2. 单击 **设置评估器**。
3. 单击右上角![](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/powerrag/langfuse/icon6.png)图标，选择默认模型。

选择的默认模型将用于所有评估器，仅自定义模版可以覆盖该设置。

说明：

+ 设置：此默认模型需要设置一次，但如果需要，可以随时更改。
+ 变化：现有的评估人员继续使用新模型进行评估——历史结果保持不变。
+ 结构化输出支持：所选的默认模型支持结构化输出至关重要。这对于我们的系统正确解读 LLM 评委的评估结果至关重要。

### 选择评估器
![](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/powerrag/langfuse/llm-judge1.png)

通过下面两种方法，选择一个评估器：

+ 内置评估器：

Langfuse 内置的多个评估器。每个评估器都针对特定质量维度（例如正确性、准确性、语境相关性、有用性）捕获最佳实践评估提示词。

    - 随时可用：无需编写提示词。
    - 不断扩展：通过添加 OSS 合作伙伴维护的评估器和未来更多的评估器类型（例如基于正则表达式）。
+ 自定义评估器：

当库不能满足您的特定需求时，请添加您自己的库：

    1. 用变量`{{variables}}`、占位符（`input`、`output`、`ground_truth` 等）编写评估提示词。
    2. 可选：自定义分数（0-1）和推理提示词以指导 LLM 评分。
    3. 可选：为此评估器固定一个自定义专用模型。如果未指定自定义模型，则将使用默认评估模型。
    4. 保存：保存后，评估器可以在您的项目中重复使用。

### 选择要评估的数据
选择模型和评估器后，现在您可以指定要对哪些数据进行评估。您可以选择在实时生产数据或数据集跑测的数据上运行。

#### 实时生产数据
评估实时生产数据可以让您实时监控 LLM 应用的性能。

+ **范围**：选择仅在新跟踪上运行，还是在现有跟踪上运行一次（用于回填）。如有疑问，我们建议在新跟踪上运行。
+ **过滤**：将评估范围缩小到您感兴趣的特定数据子集。您可以按跟踪名称、标签`userId`等进行过滤。可自由组合过滤器。
+ **预览**：界面显示与您当前过滤器匹配的过去 24 小时内的跟踪样本，让您可以检查您的选择。
+ **采样**：为了管理成本和评估吞吐量，您可以将评估器配置为在匹配跟踪的一定百分比（例如 5%）上运行。

![](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/powerrag/langfuse/llm-judge2.png)

#### 数据集跑测
LLM 作为评判评估器可以对您的跑测结果进行评分。

[通过控制台跑测](../20.跑测/30.通过控制台跑测.md)：通过控制台跑测时，您只需选择要运行的评估器即可。这些选定的评估器将自动根据您下次运行生成的数据执行。

[通过 SDK 跑测](../20.跑测/20.通过%20SDK%20跑测.md)：您可以使用 SDK 直接在代码中配置评估器。

### 映射变量并预览评估提示词
设置您的跟踪或数据集项目的哪些属性代表实际数据，以便填充这些变量，从而进行合理的评估。例如，您可以将系统记录的跟踪输入映射到提示词的`{{input}}`变量，并将 LLM 响应（即跟踪输出）映射到提示词的`{{output}}`变量。这种映射对于确保评估合理且相关至关重要。

#### 实时生产数据
+ **提示词预览**：配置映射时，界面会显示填充了实际数据的评估提示词的实时预览。此预览使用过去 24 小时内与您的过滤器匹配的历史跟踪记录。您可以浏览几个示例跟踪记录，查看它们各自的数据如何填充进提示词，从而帮助您确信映射是正确的。
+ **JSONPath**：如果数据是嵌套的（例如，在 JSON 对象内），则可以使用 JSONPath 表达式（如`$.choices[0].message.content`）来精确定位它。

![](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/powerrag/langfuse/llm-judge3.png)

#### 数据集跑测
+ **建议的映射**：系统通常能够根据数据集中的典型字段名称自动完成常见映射。例如，如果您正在评估正确性，并且提示词包含`{{input}}`、`{{output}}`和`{{ground_truth}}`变量，我们可能建议将它们分别映射到跟踪输入、跟踪输出和数据集项目的`expected_output`。
+ **编辑映射**：如果您的数据集结构不同，您可以编辑这些建议的映射。您可以映射数据集项目的任何属性（例如`input`、`expected_output`）。此外，由于数据集运行会在后台创建跟踪，因此使用跟踪输入/输出作为评估输入/输出是一种常见模式。您可以将跟踪输出视为数据集运行的输出。

## 监控和迭代
当系统评估您的数据时，会将结果写入[评分数据模型](10.评分数据模型.md)。然后您可以：

+ 查看日志：查看每次评估的详细日志，包括状态和重试错误。
+ 使用仪表板：汇总一段时间内的分数，按版本或环境进行过滤，并跟踪 LLM 应用的性能。
+ 操作：暂停、恢复或删除评估器。

![](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/powerrag/langfuse/llm-judge4.png)

## 跟踪评估器执行
每次 LLM 作为评判评估器的执行都会创建完整的追踪记录，让您全面了解评估过程。这使您能够：

+ **调试提示词问题**：查看发送给 LLM 作为评判的确切提示词。
+ **检查模型响应**：查看完整的响应，包括推理和结构化输出。
+ **监控令牌使用情况**：追踪每个评估器执行的成本和性能。
+ **跟踪评估历史**：从任何评分导航回其源 LLM 交互。

### 如何访问执行跟踪
有四种方法可以导航到评估器执行跟踪：

1. **跟踪视图中的评分**：对于 LLM 作为评判的评分，将鼠标悬停在跟踪详细信息视图中的任何评分上，然后单击 **查看执行跟踪**。
2. **跟踪列表**：过滤环境为`langfuse-llm-as-a-judge`，查看所有评估器执行跟踪。
3. **评分列表**：启用评分列表中的 **执行跟踪** 列，查看所有评估器执行的执行跟踪 ID。
4. **评估器日志表**：查看评估器日志中的执行跟踪 ID，了解详细的执行历史记录和状态。

执行跟踪显示发送给裁判的确切提示、模型的响应、令牌使用情况、延迟以及执行期间发生的任何错误。

### 了解执行状态
+ **已完成**：评估成功完成。
+ **错误**：评估失败（单击执行跟踪 ID 了解详细信息）。
+ **延迟**：LLM 提供商的评估命中率限制，正在使用指数退避算法重试。
+ **待处理**：评估正在排队等待运行。



