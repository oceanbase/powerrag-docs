# Experiments via UI

You can execute Experiments via UI (also called Prompt Experiments) in the Langfuse UI to test different prompt versions from [Prompt Management](../../30.Prompt%20Management/00.Overview.md) or language models and compare the results side-by-side.

Optionally, you can use [LLM-as-a-Judge Evaluators](../10.Evaluation%20methods/00.LLM-as-a-Judge.md) to automatically score the responses based on the expected outputs to further analyze the results on an aggregate level.

## Why use prompt experiments?

- Quickly test different prompt versions or models
- Structure your prompt testing by using a dataset to test different prompt versions and models
- Quickly iterate on prompts through Prompt Experiments
- Optionally use LLM-as-a-Judge Evaluators to score the responses based on the expected outputs from the dataset
- Prevent regressions by running tests when making prompt changes

## Prerequisites

### Create a usable prompt

Create a prompt that you want to test and evaluate. [How to create a prompt?](../../30.Prompt%20Management/10.Get%20started.md)

**A prompt is usable when:** your prompt has variables that match the dataset item keys in the dataset that will be used for the Dataset Run. 

See the example below.

#### Example: Prompt Variables & Dataset Item Keys Mapping

**Prompt:**

```bash
{{ documentation }}

Question: {{question}}

```

**Dataset Item:**

```json
{
  "documentation": "Langfuse is an LLM Engineering Platform",
  "question": "What is Langfuse?"
}
```

In this example:

- The prompt variable `{{documentation}}` maps to the JSON key `"documentation"`
- The prompt variable `{{question}}` maps to the JSON key `"question"`
- Both keys must exist in the dataset item's input JSON for the experiment to run successfully

#### Example: Chat Message Placeholder Mapping</summary>

In addition to variables, you can also map placeholders in chat message prompts to dataset item keys.
This is useful when the dataset item also contains for example a chat message history to use.
Your chat prompt needs to contain a placeholder with a name. Variables within placeholders are not resolved.

**Chat Prompt:**
Placeholder named: `message_history`

**Dataset Item:**

```json
{
  "message_history": [
    {
      "role": "user",
      "content": "What is Langfuse?"
    },
    {
      "role": "assistant",
      "content": "Langfuse is a tool for tracking and analyzing the performance of language models."
    }
  ],
  "question": "What is Langfuse?"
}
```

In this example:

- The chat prompt placeholder `message_history` maps to the JSON key `"message_history"`.
- The prompt variable `{{question}}` maps to the JSON key `"question"` in a variable not within a placeholder message.
- Both keys must exist in the dataset item's input JSON for the experiment to run successfully

### Create a usable dataset

Create a dataset with the inputs and expected outputs you want to use for your prompt experiments. [How to create a dataset?](10.Datasets.md)

**A dataset is usable when:** 
+ The dataset items have JSON objects as input
+ These objects have JSON keys that match the prompt variables of the prompt(s) you will use. 

See the example below.

#### Example: Prompt Variables & Dataset Item Keys Mapping</summary>

**Prompt:**

```bash
{{ documentation }}

Question: {{question}}

```

**Dataset Item:**

```json
{
  "documentation": "Langfuse is an LLM Engineering Platform",
  "question": "What is Langfuse?"
}
```

In this example:

- The prompt variable `{{documentation}}` maps to the JSON key `"documentation"`
- The prompt variable `{{question}}` maps to the JSON key `"question"`
- Both keys must exist in the dataset item's input JSON for the experiment to run successfully

### Configure LLM connection

As your prompt will be executed for each dataset item, you need to configure an LLM connection in the project settings. [How to configure an LLM connection?](../30.Configure%20an%20LLM%20connection.md)

### Optional: Set up LLM-as-a-judge

You can set up an LLM-as-a-judge evaluator to score the responses based on the expected outputs. Make sure to set the target of the LLM-as-a-Judge to "Experiment runs" and filter for the dataset you want to use. [How to set up LLM-as-a-judge?](../10.Evaluation%20methods/00.LLM-as-a-Judge.md)

## Trigger an experiment via UI

### Start a dataset run

1. In the left-side navigation pane, click **Datasets**. 
2. Click the dataset name to go to the dataset details page.
3. Click **New dataset run** at the top of the page.
4. Click **Configure** at the bottom of the **Via User Interface** card in the pop-up.
5. Enter the dataset run name and description.
6. Select the prompt, dataset, model, and evaluator you want to use.
7. Click **Start** to trigger the dataset run.

You will be redirected to the Dataset Runs page. The run might take a few seconds or minutes to complete depending on the prompt complexity and dataset size.

### Compare runs

After each experiment run, you can check the aggregated score in the Dataset Runs table and compare results side-by-side.

1. In the left-side navigation pane, click **Datasets**. 
2. Click the dataset name to go to the dataset details page.
3. Select multiple runs in the runs list.
4. Click **Actions** > **Compare** on the right to compare the selected runs side-by-side.
